{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'torch'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_29538/682197674.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mneural_network\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mMLPRegressor\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlinear_model\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mLinearRegression\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmetrics\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0maccuracy_score\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprecision_score\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrecall_score\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mf1_score\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'torch'"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "from sklearn.neural_network import MLPRegressor\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "\n",
    "from transformer_lens import HookedTransformer, HookedTransformerConfig\n",
    "import transformer_lens.utils as utils\n",
    "\n",
    "from tree_generation import *\n",
    "from utils import *\n",
    "from interp_utils import *\n",
    "from probing import *\n",
    "from sparse_coding import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_examples = 300_000\n",
    "n_states = 16\n",
    "\n",
    "dataset = GraphDataset(n_states, \"dataset.txt\", n_examples)\n",
    "dataset.visualize_example(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cfg = HookedTransformerConfig(\n",
    "    n_layers=6,\n",
    "    d_model=128,\n",
    "    n_ctx=dataset.max_seq_length - 1,\n",
    "    n_heads=1,\n",
    "    d_mlp=512,\n",
    "    d_head=128,\n",
    "    #attn_only=True,\n",
    "    d_vocab=len(dataset.idx2tokens),\n",
    "    device=\"cuda\",\n",
    "    attention_dir= \"causal\",\n",
    "    act_fn=\"gelu\",\n",
    ")\n",
    "model = HookedTransformer(cfg)\n",
    "\n",
    "\n",
    "# Load in the model if weights are in the directory, else train new model\n",
    "if os.path.exists(\"model.pt\"):\n",
    "    model.load_state_dict(torch.load(\"model.pt\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "random_seed = np.random.randint(1_000_000, 1_000_000_000)\n",
    "pred = generate_example(n_states, random_seed, order=\"backward\")\n",
    "if is_model_correct(model, dataset, pred):\n",
    "    parse_example(pred)\n",
    "labels,cache = get_example_cache(pred, model, dataset)\n",
    "labels= [f'N{i}_{component}' for i, component in enumerate(labels)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for l in range(model.cfg.n_layers):\n",
    "    for h in range(model.cfg.n_heads):\n",
    "        fig = display_head(cache, labels, l, h, show=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "random_seed = np.random.randint(1_000_000, 1_000_000_000)\n",
    "pred = \"14>15,0>2,7>14,5>6,5>0,4>7,12>13,8>5,8>4,9>12,9>8,1>3,10>9,10>1,11>10|6:11>10>9>8>5>6\"\n",
    "if is_model_correct(model, dataset, pred):\n",
    "    parse_example(pred)\n",
    "labels,cache = get_example_cache(pred, model, dataset)\n",
    "labels= [f'N{i}_{component}' for i, component in enumerate(labels)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "for l in range(model.cfg.n_layers):\n",
    "    for h in range(model.cfg.n_heads):\n",
    "        fig = display_head(cache, labels, l, h, show=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "clean_prompt = \"0>1,1>2,2>3,3>4,4>5,5>6,6>7,7>8,8>9,9>10,10>11,11>12,12>13,13>14,14>15|15:0>1>2>3>4>5>6>7>8>9>10>11>12>13>14>15\"\n",
    "\n",
    "corrupted_prompt = \"0>1,1>2,2>3,3>4,4>5,4>6,5>7,7>8,8>9,9>10,10>11,11>12,12>13,13>14,14>15|15:0>1>2>3>4>5>7>8>9>10>11>12>13>14>15\"\n",
    "\n",
    "parse_example(clean_prompt)\n",
    "plt.show()\n",
    "parse_example(corrupted_prompt)\n",
    "plt.show()\n",
    "# Tokenize\n",
    "clean_tokens = torch.from_numpy(dataset.tokenize(clean_prompt)[0:-1]).cuda()\n",
    "corrupted_tokens = torch.from_numpy(dataset.tokenize(corrupted_prompt)[0:-1]).cuda()\n",
    "\n",
    "\n",
    "clean_prompt_backwards = \"14>15,13>14,12>13,11>12,10>11,9>10,8>9,7>8,6>7,5>6,4>5,3>4,2>3,1>2,0>1|15:0>1>2>3>4>5>6>7>8>9>10>11>12>13>14>15\"\n",
    "\n",
    "corrupted_prompt_backwards = \"14>15,13>14,12>13,11>12,10>11,9>10,8>9,7>8,5>7,4>6,4>5,3>4,2>3,1>2,0>1|15:0>1>2>3>4>5>7>8>9>10>11>12>13>14>15\"\n",
    "\n",
    "parse_example(clean_prompt_backwards)\n",
    "plt.show()\n",
    "parse_example(corrupted_prompt_backwards)\n",
    "plt.show()\n",
    "# Tokenize\n",
    "clean_tokens_backwards  = torch.from_numpy(dataset.tokenize(clean_prompt_backwards )[0:-1]).cuda()\n",
    "corrupted_tokens_backwards  = torch.from_numpy(dataset.tokenize(corrupted_prompt_backwards )[0:-1]).cuda()\n",
    "\n",
    "patching_result = activation_patching(model, dataset, clean_tokens, corrupted_tokens, 46 + 7 )\n",
    "patching_result_backwards  = activation_patching(model, dataset, clean_tokens_backwards , corrupted_tokens_backwards , 46 + 7 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_activations(patching_result,clean_tokens,dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_activations(patching_result_backwards,clean_tokens_backwards,dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def activation_patching_register(model, dataset, clean_tokens, corrupted_tokens, comparison_index,positions):\n",
    "    # We run on the clean prompt with the cache so we store activations to patch in later.\n",
    "    clean_logits, clean_cache = model.run_with_cache(clean_tokens)\n",
    "    clean_logit_diff = logits_to_logit_diff(clean_tokens, corrupted_tokens, clean_logits, comparison_index)\n",
    "    print(f\"Clean logit difference: {clean_logit_diff.item():.3f}\")\n",
    "\n",
    "    # We don't need to cache on the corrupted prompt.\n",
    "    corrupted_logits = model(corrupted_tokens)\n",
    "    corrupted_logit_diff = logits_to_logit_diff(clean_tokens, corrupted_tokens, corrupted_logits, comparison_index)\n",
    "    print(f\"Corrupted logit difference: {corrupted_logit_diff.item():.3f}\")\n",
    "    print(f\"Positive Direction: {dataset.idx2tokens[clean_tokens[comparison_index]]}\")\n",
    "    print(f\"Negative Direction: {dataset.idx2tokens[corrupted_tokens[comparison_index]]}\")\n",
    "\n",
    "    def residual_stream_patching_hook(\n",
    "        resid_pre,\n",
    "        hook,\n",
    "        positions):\n",
    "        # Each HookPoint has a name attribute giving the name of the hook.\n",
    "        clean_resid_pre = clean_cache[hook.name]\n",
    "        for position in positions:\n",
    "            resid_pre[:, position, :] = clean_resid_pre[:, position, :]\n",
    "        return resid_pre\n",
    "    # We make a tensor to store the results for each patching run. We put it on the model's device to avoid needing to move things between the GPU and CPU, which can be slow.\n",
    "    num_positions = clean_tokens.shape[0]\n",
    "    patching_result = torch.zeros((model.cfg.n_layers), device=model.cfg.device)\n",
    "    for layer in tqdm_auto.tqdm(range(model.cfg.n_layers)):\n",
    "        # Use functools.partial to create a temporary hook function with the position fixed\n",
    "        temp_hook_fn = partial(residual_stream_patching_hook, positions=positions)\n",
    "        # Run the model with the patching hook\n",
    "        patched_logits = model.run_with_hooks(corrupted_tokens, fwd_hooks=[\n",
    "            (tl_util.get_act_name(\"resid_pre\", layer), temp_hook_fn)\n",
    "        ])\n",
    "        # Calculate the logit difference\n",
    "        patched_logit_diff = logits_to_logit_diff(clean_tokens, corrupted_tokens, patched_logits, comparison_index).detach()\n",
    "        # Store the result, normalizing by the clean and corrupted logit difference so it's between 0 and 1 (ish)\n",
    "        normalize_ratio = (clean_logit_diff - corrupted_logit_diff)\n",
    "        if normalize_ratio == 0:\n",
    "            normalize_ratio = 1\n",
    "        patching_result[layer] = (patched_logit_diff - corrupted_logit_diff) / normalize_ratio\n",
    "    return patching_result\n",
    "\n",
    "\n",
    "def plot_activations(patching_result, clean_tokens, dataset):\n",
    "    # Add the index to the end of the label, because plotly doesn't like duplicate labels\n",
    "    token_labels = [f\"{dataset.idx2tokens[token]}_{index}\" for index, token in enumerate(clean_tokens)]\n",
    "    imshow(patching_result, x=token_labels, xaxis=\"Position\", yaxis=\"Layer\", title=\"Activation patching\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def activation_patching_position_and_layers(model, dataset, clean_tokens, corrupted_tokens, comparison_index,positions,layers):\n",
    "    # We run on the clean prompt with the cache so we store activations to patch in later.\n",
    "    clean_logits, clean_cache = model.run_with_cache(clean_tokens)\n",
    "    clean_logit_diff = logits_to_logit_diff(clean_tokens, corrupted_tokens, clean_logits, comparison_index)\n",
    "    print(f\"Clean logit difference: {clean_logit_diff.item():.3f}\")\n",
    "\n",
    "    # We don't need to cache on the corrupted prompt.\n",
    "    corrupted_logits = model(corrupted_tokens)\n",
    "    corrupted_logit_diff = logits_to_logit_diff(clean_tokens, corrupted_tokens, corrupted_logits, comparison_index)\n",
    "    print(f\"Corrupted logit difference: {corrupted_logit_diff.item():.3f}\")\n",
    "    print(f\"Positive Direction: {dataset.idx2tokens[clean_tokens[comparison_index]]}\")\n",
    "    print(f\"Negative Direction: {dataset.idx2tokens[corrupted_tokens[comparison_index]]}\")\n",
    "\n",
    "    def residual_stream_patching_hook(\n",
    "        resid_pre,\n",
    "        hook,\n",
    "        positions):\n",
    "        # Each HookPoint has a name attribute giving the name of the hook.\n",
    "        clean_resid_pre = clean_cache[hook.name]\n",
    "        for position in positions:\n",
    "            resid_pre[:, position, :] = clean_resid_pre[:, position, :]\n",
    "        return resid_pre\n",
    "    # We make a tensor to store the results for each patching run. We put it on the model's device to avoid needing to move things between the GPU and CPU, which can be slow.\n",
    "    num_positions = clean_tokens.shape[0]\n",
    "    patching_result = 0\n",
    "    # Use functools.partial to create a temporary hook function with the position fixed\n",
    "    temp_hook_fn = partial(residual_stream_patching_hook, positions=positions)\n",
    "    # Run the model with the patching hook\n",
    "    hooks=[]\n",
    "    for layer in layers:\n",
    "        hooks.append((tl_util.get_act_name(\"resid_pre\", layer), temp_hook_fn))\n",
    "\n",
    "    patched_logits = model.run_with_hooks(corrupted_tokens, fwd_hooks=\n",
    "            hooks\n",
    "        )   \n",
    "     \n",
    "    # Calculate the logit difference\n",
    "    patched_logit_diff = logits_to_logit_diff(clean_tokens, corrupted_tokens, patched_logits, comparison_index).detach()\n",
    "    # Store the result, normalizing by the clean and corrupted logit difference so it's between 0 and 1 (ish)\n",
    "    patching_result=0\n",
    "    normalize_ratio = (clean_logit_diff - corrupted_logit_diff)\n",
    "    if normalize_ratio == 0:\n",
    "        normalize_ratio = 1\n",
    "    patching_result = (patched_logit_diff - corrupted_logit_diff) / normalize_ratio\n",
    "    return patching_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.reset_hooks()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clean_tokens = torch.from_numpy(dataset.tokenize(clean_prompt)[0:-1]).cuda()\n",
    "corrupted_tokens = torch.from_numpy(dataset.tokenize(corrupted_prompt)[0:-1]).cuda()\n",
    "patching_result = activation_patching_register(model, dataset, clean_tokens, corrupted_tokens, 46 + 7 ,[36,38,39,41,42,44,45])\n",
    "patching_result_backwards = activation_patching_register(model, dataset, clean_tokens_backwards, corrupted_tokens_backwards, 46 + 7 ,[36,38,39,41,42,44,45])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "imshow([patching_result])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "imshow([patching_result_backwards])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "patching_result_backwards_layers=activation_patching_position_and_layers(model, dataset, clean_tokens_backwards, corrupted_tokens_backwards, 46 + 7 ,[36,38,39,41,42,44,45],[0,1,2,3,4,5])#[36,38,39,41,42,44,45],[0,1,2,3,4,5])\n",
    "patching_result_backwards_layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "patching_result_backwards_layers=activation_patching_position_and_layers(model, dataset, clean_tokens_backwards, corrupted_tokens_backwards, 46 + 7 ,[36,38,39,41,42,44,45,46,47],[0,1,2,3,4,5])#[36,38,39,41,42,44,45],[0,1,2,3,4,5])\n",
    "patching_result_backwards_layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def activation_patching_layers(model, dataset, clean_tokens, corrupted_tokens, comparison_index,layers):\n",
    "    # We run on the clean prompt with the cache so we store activations to patch in later.\n",
    "    clean_logits, clean_cache = model.run_with_cache(clean_tokens)\n",
    "    clean_logit_diff = logits_to_logit_diff(clean_tokens, corrupted_tokens, clean_logits, comparison_index)\n",
    "    print(f\"Clean logit difference: {clean_logit_diff.item():.3f}\")\n",
    "\n",
    "    # We don't need to cache on the corrupted prompt.\n",
    "    corrupted_logits = model(corrupted_tokens)\n",
    "    corrupted_logit_diff = logits_to_logit_diff(clean_tokens, corrupted_tokens, corrupted_logits, comparison_index)\n",
    "    print(f\"Corrupted logit difference: {corrupted_logit_diff.item():.3f}\")\n",
    "    print(f\"Positive Direction: {dataset.idx2tokens[clean_tokens[comparison_index]]}\")\n",
    "    print(f\"Negative Direction: {dataset.idx2tokens[corrupted_tokens[comparison_index]]}\")\n",
    "\n",
    "    def residual_stream_patching_hook(\n",
    "        resid_pre,\n",
    "        hook,\n",
    "        position):\n",
    "        # Each HookPoint has a name attribute giving the name of the hook.\n",
    "        clean_resid_pre = clean_cache[hook.name]\n",
    "        resid_pre[:, position, :] = clean_resid_pre[:, position, :]\n",
    "        return resid_pre\n",
    "    # We make a tensor to store the results for each patching run. We put it on the model's device to avoid needing to move things between the GPU and CPU, which can be slow.\n",
    "    num_positions = clean_tokens.shape[0]\n",
    "    patching_result = torch.zeros(num_positions, device=model.cfg.device)\n",
    "    for position in range(num_positions):\n",
    "            # Use functools.partial to create a temporary hook function with the position fixed\n",
    "            temp_hook_fn = partial(residual_stream_patching_hook, position=position)\n",
    "            # Run the model with the patching hook\n",
    "            hooks=[]\n",
    "            for layer in layers:\n",
    "                hooks.append((tl_util.get_act_name(\"resid_pre\", layer), temp_hook_fn))\n",
    "            \n",
    "            patched_logits = model.run_with_hooks(corrupted_tokens, fwd_hooks=hooks)\n",
    "            # Calculate the logit difference\n",
    "            patched_logit_diff = logits_to_logit_diff(clean_tokens, corrupted_tokens, patched_logits, comparison_index).detach()\n",
    "            # Store the result, normalizing by the clean and corrupted logit difference so it's between 0 and 1 (ish)\n",
    "            normalize_ratio = (clean_logit_diff - corrupted_logit_diff)\n",
    "            if normalize_ratio == 0:\n",
    "                normalize_ratio = 1\n",
    "            patching_result[position] = (patched_logit_diff - corrupted_logit_diff) / normalize_ratio\n",
    "    return patching_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def delete_non_paths(input_dict):\n",
    "    keys_to_delete = [key for key, value in input_dict.items() if len(value) <= 2]\n",
    "    for key in keys_to_delete:\n",
    "        del input_dict[key]\n",
    "    return input_dict\n",
    "\n",
    "special_chars = [\",\", \":\", \"|\"]\n",
    "def get_paths(cache, labels, threshold=0.6):\n",
    "\n",
    "    paths = {}\n",
    "    for layer in range(1, 6):\n",
    "        attn_pattern = cache[f\"blocks.{layer}.attn.hook_pattern\"]\n",
    "        _, _, seq_len, _ = attn_pattern.shape\n",
    "\n",
    "        for current_pos in range(seq_len):       \n",
    "            current_token = labels[current_pos]     \n",
    "            for attended_pos in range(seq_len):\n",
    "                attn_value = attn_pattern[0, 0, current_pos, attended_pos]\n",
    "                if attn_value > threshold:\n",
    "                    attended_token = labels[attended_pos].replace(\">\", \"\")\n",
    "                    previous_token = labels[attended_pos - 1].replace(\">\", \"\")\n",
    "                    if not attended_token in special_chars and not previous_token in special_chars:\n",
    "                        identifier = (current_pos, current_token)\n",
    "                        if identifier in paths.keys():\n",
    "                            paths[identifier].append(previous_token)\n",
    "                        else:\n",
    "                            paths[identifier] = [attended_token, previous_token]\n",
    "    \n",
    "    paths = delete_non_paths(paths)\n",
    "    return paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#clean_prompt_test=\"8>14,8>9,5>11,5>2,10>8,10>5,1>4,6>15,6>10,0>1,12>6,12>0,3>13,7>12,7>3|11:7>12>6>10>5>11\"\n",
    "#corrupted_prompt_test=\"8>14,8>9,5>11,5>2,10>8,10>5,1>4,6>15,6>10,0>1,3>6,3>0,12>13,7>3,7>12|11:7>3>6>10>5>11\"\n",
    "#clean_prompt_test=\"15>14,8>9,5>11,5>2,10>8,10>5,1>4,7>15,6>10,0>1,12>6,12>0,3>13,7>12,7>3|11:7>12>6>10>5>11\"\n",
    "#corrupted_prompt_test=\"15>14,8>9,5>11,5>2,10>8,10>5,1>4,7>15,6>10,0>1,3>6,3>0,12>13,7>3,7>12|11:7>3>6>10>5>11\"\n",
    "clean_prompt_test=\"15>14,7>9,5>11,9>2,10>8,10>5,1>4,7>15,6>10,0>1,12>6,12>0,3>13,7>12,7>3|11:7>12>6>10>5>11\"\n",
    "corrupted_prompt_test=\"15>14,7>9,5>11,9>2,10>8,10>5,1>4,7>15,6>10,0>1,3>6,3>0,12>13,7>3,7>12|11:7>3>6>10>5>11\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'parse_example' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_29538/3286463035.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mparse_example\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mclean_prompt_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mparse_example\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcorrupted_prompt_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'parse_example' is not defined"
     ]
    }
   ],
   "source": [
    "parse_example(clean_prompt_test)\n",
    "plt.show()\n",
    "parse_example(corrupted_prompt_test)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clean_tokens_backwards_test = torch.from_numpy(dataset.tokenize(clean_prompt_test)[0:-1]).cuda()\n",
    "corrupted_tokens_backwards_test= torch.from_numpy(dataset.tokenize(corrupted_prompt_test)[0:-1]).cuda()\n",
    "labels, cache = get_example_cache(clean_prompt_test, model, dataset)\n",
    "subpaths_clean = get_paths(cache, labels)\n",
    "print(f'subpaths clean:{subpaths_clean}')\n",
    "labels, cache = get_example_cache(corrupted_prompt_test, model, dataset)\n",
    "subpaths_corrupted = get_paths(cache, labels)\n",
    "print(f'subpaths corrupted:{subpaths_corrupted}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clean_tokens_backwards_test = torch.from_numpy(dataset.tokenize(clean_prompt_test)[0:-1]).cuda()\n",
    "corrupted_tokens_backwards_test= torch.from_numpy(dataset.tokenize(corrupted_prompt_test)[0:-1]).cuda()\n",
    "register_patching_result_backwards_test = activation_patching_register(model, dataset, clean_tokens_backwards_test, corrupted_tokens_backwards_test, 46 + 2 ,[36,38,39,41,42,44,45])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "imshow(model(clean_tokens_backwards_test)[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "imshow(model(corrupted_tokens_backwards_test)[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.reset_hooks()\n",
    "clean_logits, clean_cache = model.run_with_cache(clean_tokens_backwards_test)\n",
    "def residual_stream_patching_hook(\n",
    "        resid_pre,\n",
    "        hook,\n",
    "        positions):\n",
    "        # Each HookPoint has a name attribute giving the name of the hook.\n",
    "        clean_resid_pre = clean_cache[hook.name]\n",
    "        for position in positions:\n",
    "            resid_pre[:, position, :] = clean_resid_pre[:, position, :]\n",
    "        return resid_pre\n",
    "    # We make a tensor to store the results for each patching run. We put it on the model's device to avoid needing to move things between the GPU and CPU, which can be slow.\n",
    "\n",
    "temp_hook = partial(residual_stream_patching_hook, positions=[38])#[36,38,39,41,42,44,45])\n",
    "# Run the model with the patching hook\n",
    "patched_logits = model.run_with_hooks(corrupted_tokens_backwards_test, fwd_hooks=[\n",
    "            (tl_util.get_act_name(\"resid_pre\", 3), temp_hook),(tl_util.get_act_name(\"resid_pre\", 4), temp_hook)\n",
    "        ])\n",
    "model.reset_hooks()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "imshow(patched_logits[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "imshow(torch.softmax(patched_logits[0],1)[47:49])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "imshow(clean_logits[0][47:49])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "imshow(torch.softmax(clean_logits[0],1)[47:49])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "imshow((torch.softmax(clean_logits[0],1)-torch.softmax(patched_logits[0],1))[47:49])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "register_patching_result_backwards_test = activation_patching_register(model, dataset, clean_tokens_backwards_test, corrupted_tokens_backwards_test, 46 + 2 ,[36,38,39,41,42,44,45])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "imshow([register_patching_result_backwards_test])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "patching_result_backwards_test = activation_patching(model, dataset, clean_tokens_backwards_test, corrupted_tokens_backwards_test, 46 + 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_activations(patching_result_backwards_test, clean_tokens_backwards_test, dataset)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "patching_result_backwards_layers=activation_patching_layers(model, dataset, clean_tokens_backwards, corrupted_tokens_backwards, 46 + 7 ,[0,1,2,3,4,5])#[36,38,39,41,42,44,45],[0,1,2,3,4,5])\n",
    "plot_activations([patching_result_backwards_layers], clean_tokens, dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Patching Result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def replace_nodes(graph,n1,n2):\n",
    "    replace_A_1=graph.replace(f\">{n1},\",\">A,\")\n",
    "    replace_A_2=replace_A_1.replace(f\",{n1}>\",\",A>\")\n",
    "    replace_A_3=replace_A_2.replace(f\">{n1}>\",\">A>\")\n",
    "    replace_A_4=replace_A_3.replace(f\">{n1}|\",\">A|\")\n",
    "    replace_n1_1=replace_A_4.replace(f\">{n2},\",f\">{n1},\")\n",
    "    replace_n1_2=replace_n1_1.replace(f\",{n2}>\",f\",{n1}>\")\n",
    "    replace_n1_3=replace_n1_2.replace(f\">{n2}>\",f\">{n1}>\")\n",
    "    replace_n1_4=replace_n1_3.replace(f\">{n2}|\",f\">{n1}|\")\n",
    "    replace_n2=replace_n1_4.replace(\"A\",f\"{n2}\")\n",
    "    return replace_n2\n",
    "    #Doesnt replace first node in list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "random_seed = np.random.randint(1_000_000, 1_000_000_000)\n",
    "graph = generate_example(n_states, random_seed, order=\"backward\")\n",
    "full_path = graph.split(\":\")[1].split(\">\")[1:]  # we ignore the first position, might need to reconsider this at some point\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'7>2,10>7,14>10,8>14,0>8,6>0,9>6,11>9,3>11,12>3,1>12,15>1,5>13,4>15,4>5|2:4>15>1>12>3>11>9>6>0>8>14>10>7>2'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "parse_example(graph)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corrupted_graph= replace_nodes(graph,13,10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corrupted_graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "parse_example(graph)\n",
    "plt.show()\n",
    "parse_example(corrupted_graph)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.reset_hooks()\n",
    "position=46 + 5\n",
    "clean_graph_tokens = torch.from_numpy(dataset.tokenize(graph)[0:-1]).cuda()\n",
    "clean_logits, clean_cache = model.run_with_cache(clean_graph_tokens)\n",
    "print( dataset.untokenize(np.argmax(clean_logits.detach().cpu(),2)[0][47:]))\n",
    "corrupted_graph_tokens= torch.from_numpy(dataset.tokenize(corrupted_graph)[0:-1]).cuda()\n",
    "corrupted_logits, corrupted_cache = model.run_with_cache(corrupted_graph_tokens)\n",
    "activation_patching_result=activation_patching(model, dataset, clean_graph_tokens,corrupted_graph_tokens , position)#46 + 2\n",
    "register_pathcing_result= activation_patching_register(model, dataset, clean_graph_tokens,corrupted_graph_tokens , position,[36,38,39,41,42,44,45])\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels, cache = get_example_cache(graph, model, dataset)\n",
    "get_paths(clean_cache,labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "imshow([register_pathcing_result[1:]],x=list(range(1,model.cfg.n_layers)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "imshow([register_pathcing_result[1:]],x=list(range(1,model.cfg.n_layers)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_activations(activation_patching_result,clean_graph_tokens, dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_activations(activation_patching_result,clean_graph_tokens, dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test leaf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "graph_not_leaf='7>2,10>7,14>10,8>14,0>8,6>0,9>6,11>9,3>11,12>3,1>12,15>1,5>13,4>15,4>5|2:4>15>1>12>3>11>9>6>0>8>14>10>7>2'\n",
    "graph_leaf='7>2,10>7,14>10,8>14,0>8,6>0,9>6,11>9,3>11,12>3,1>12,15>1,4>13,4>15,4>5|2:4>15>1>12>3>11>9>6>0>8>14>10>7>2'\n",
    "\n",
    "corrupted_graph_not_leaf= replace_nodes(graph_not_leaf,15,5)\n",
    "corrupted_graph_leaf= replace_nodes(graph_leaf,15,5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "parse_example(graph_not_leaf)\n",
    "plt.show()\n",
    "parse_example(graph_leaf)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "parse_example(corrupted_graph_not_leaf)\n",
    "plt.show()\n",
    "parse_example(corrupted_graph_leaf)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.reset_hooks()\n",
    "position=46 + 2\n",
    "clean_graph_leaf_tokens = torch.from_numpy(dataset.tokenize(graph_leaf)[0:-1]).cuda()\n",
    "clean_logits_leaf, clean_cache_leaf = model.run_with_cache(clean_graph_leaf_tokens)\n",
    "print( dataset.untokenize(np.argmax(clean_logits_leaf.detach().cpu(),2)[0][47:]))\n",
    "corrupted_graph_leaf_tokens= torch.from_numpy(dataset.tokenize(corrupted_graph_leaf)[0:-1]).cuda()\n",
    "corrupted_logits_leaf, corrupted_cache_leaf = model.run_with_cache(corrupted_graph_leaf_tokens)\n",
    "activation_patching_result_leaf=activation_patching(model, dataset, clean_graph_leaf_tokens,corrupted_graph_leaf_tokens , position)#46 + 2\n",
    "register_pathcing_result_leaf= activation_patching_register(model, dataset, clean_graph_leaf_tokens,corrupted_graph_leaf_tokens , position,[36,38,39,41,42,44,45])\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_activations(activation_patching_result_leaf,clean_graph_leaf_tokens, dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "imshow([register_pathcing_result_leaf[1:]],x=list(range(1,model.cfg.n_layers)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.reset_hooks()\n",
    "position=46 + 2  \n",
    "clean_graph_not_leaf_tokens = torch.from_numpy(dataset.tokenize(graph_not_leaf)[0:-1]).cuda()\n",
    "clean_logits_not_leaf, clean_cache_not_leaf = model.run_with_cache(clean_graph_not_leaf_tokens)\n",
    "print( dataset.untokenize(np.argmax(clean_logits_not_leaf.detach().cpu(),2)[0][47:]))\n",
    "corrupted_graph_not_leaf_tokens= torch.from_numpy(dataset.tokenize(corrupted_graph_not_leaf)[0:-1]).cuda()\n",
    "corrupted_logits_not_leaf, corrupted_cache_not_leaf = model.run_with_cache(corrupted_graph_not_leaf_tokens)\n",
    "activation_patching_result_not_leaf=activation_patching(model, dataset, clean_graph_not_leaf_tokens,corrupted_graph_not_leaf_tokens , position)#46 + 2\n",
    "register_pathcing_result_not_leaf= activation_patching_register(model, dataset, clean_graph_not_leaf_tokens,corrupted_graph_not_leaf_tokens , position,[36,38,39,41,42,44,45])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_activations(activation_patching_result_not_leaf,clean_graph_leaf_tokens, dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "imshow([register_pathcing_result_not_leaf[1:]],x=list(range(1,model.cfg.n_layers)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "register_pathcing_result_not_leaf_test_46= activation_patching_register(model, dataset, clean_graph_not_leaf_tokens,corrupted_graph_not_leaf_tokens , position,[36,38,39,41,42,44,45,46])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "imshow([register_pathcing_result_not_leaf_test_46[1:]],x=list(range(1,model.cfg.n_layers)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test choices "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def activation_patching_logits(model, dataset, clean_tokens, corrupted_tokens, comparison_index):\n",
    "    # We run on the clean prompt with the cache so we store activations to patch in later.\n",
    "    clean_logits, clean_cache = model.run_with_cache(clean_tokens)\n",
    "    clean_logit_diff = logits_to_logit_diff(clean_tokens, corrupted_tokens, clean_logits, comparison_index)\n",
    "    print(f\"Clean logit difference: {clean_logit_diff.item():.3f}\")\n",
    "\n",
    "    # We don't need to cache on the corrupted prompt.\n",
    "    corrupted_logits = model(corrupted_tokens)\n",
    "    corrupted_logit_diff = logits_to_logit_diff(clean_tokens, corrupted_tokens, corrupted_logits, comparison_index)\n",
    "    print(f\"Corrupted logit difference: {corrupted_logit_diff.item():.3f}\")\n",
    "    print(f\"Positive Direction: {dataset.idx2tokens[clean_tokens[comparison_index]]}\")\n",
    "    print(f\"Negative Direction: {dataset.idx2tokens[corrupted_tokens[comparison_index]]}\")\n",
    "\n",
    "    def residual_stream_patching_hook(\n",
    "        resid_pre,\n",
    "        hook,\n",
    "        position):\n",
    "        # Each HookPoint has a name attribute giving the name of the hook.\n",
    "        clean_resid_pre = clean_cache[hook.name]\n",
    "        resid_pre[:, position, :] = clean_resid_pre[:, position, :]\n",
    "        return resid_pre\n",
    "    # We make a tensor to store the results for each patching run. We put it on the model's device to avoid needing to move things between the GPU and CPU, which can be slow.\n",
    "    num_positions = clean_tokens.shape[0]\n",
    "    patching_result = torch.zeros((model.cfg.n_layers, num_positions), device=model.cfg.device)\n",
    "    for layer in tqdm_auto.tqdm(range(model.cfg.n_layers)):\n",
    "        for position in range(num_positions):\n",
    "            # Use functools.partial to create a temporary hook function with the position fixed\n",
    "            temp_hook_fn = partial(residual_stream_patching_hook, position=position)\n",
    "            # Run the model with the patching hook\n",
    "            patched_logits = model.run_with_hooks(corrupted_tokens, fwd_hooks=[\n",
    "                (tl_util.get_act_name(\"resid_pre\", layer), temp_hook_fn)\n",
    "            ])\n",
    "            # Calculate the logit difference\n",
    "            patched_logit_diff = logits_to_logit_diff(clean_tokens, corrupted_tokens, patched_logits, comparison_index).detach()\n",
    "            # Store the result, normalizing by the clean and corrupted logit difference so it's between 0 and 1 (ish)\n",
    "            normalize_ratio = 1\n",
    "            if normalize_ratio == 0:\n",
    "                normalize_ratio = 1\n",
    "            patching_result[layer, position] = (patched_logit_diff - corrupted_logit_diff) / normalize_ratio\n",
    "    return patching_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#graph_choices='7>2,10>7,8>10,6>8,9>6,11>9,3>11,12>3,1>12,13>14,15>1,4>13,4>15,5>0,4>5|2:4>15>1>12>3>11>9>6>8>10>7>2'\n",
    "#corrupted_graph_choices= replace_nodes(graph_choices,15,5)\n",
    "#graph_choices='7>2,10>7,8>10,6>8,9>6,11>9,3>11,12>3,1>12,13>14,15>1,0>13,4>15,5>0,4>5|2:4>15>1>12>3>11>9>6>8>10>7>2'\n",
    "#corrupted_graph_choices= replace_nodes(graph_choices,15,5)\n",
    "graph_choices='7>2,10>7,8>10,6>8,9>6,11>0,0>9,3>11,12>3,1>12,13>14,15>1,4>13,4>15,4>5|2:4>15>1>12>3>11>0>9>6>8>10>7>2'\n",
    "corrupted_graph_choices= replace_nodes(graph_choices,15,5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "parse_example(graph_choices)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'model' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_29538/1743847003.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreset_hooks\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mposition\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m46\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mclean_graph_choices_tokens\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_numpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtokenize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgraph_choices\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mclean_logits_choices\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mclean_cache_choices\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun_with_cache\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mclean_graph_choices_tokens\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m \u001b[0mdataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0muntokenize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mclean_logits_choices\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdetach\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcpu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m47\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'model' is not defined"
     ]
    }
   ],
   "source": [
    "model.reset_hooks()\n",
    "position=46 + 2  \n",
    "clean_graph_choices_tokens = torch.from_numpy(dataset.tokenize(graph_choices)[0:-1]).cuda()\n",
    "clean_logits_choices, clean_cache_choices = model.run_with_cache(clean_graph_choices_tokens)\n",
    "print( dataset.untokenize(np.argmax(clean_logits_choices.detach().cpu(),2)[0][47:]))\n",
    "corrupted_graph_choices_tokens= torch.from_numpy(dataset.tokenize(corrupted_graph_choices)[0:-1]).cuda()\n",
    "corrupted_logits_choices, corrupted_cache_choices = model.run_with_cache(corrupted_graph_choices_tokens)\n",
    "activation_patching_result_choices=activation_patching(model, dataset, clean_graph_choices_tokens,corrupted_graph_choices_tokens , position)#46 + 2\n",
    "activation_patching_result_choices_logits=activation_patching_logits(model, dataset, clean_graph_choices_tokens,corrupted_graph_choices_tokens , position)#46 + 2\n",
    "register_pathcing_result_choices= activation_patching_register(model, dataset, clean_graph_choices_tokens,corrupted_graph_choices_tokens , position,[36,38,39,41,42,44,45])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "imshow(clean_logits_choices[0][47:48],x=dataset.idx2tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "imshow(corrupted_logits_choices[0][47:48],x=dataset.idx2tokens)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels, cache = get_example_cache(graph_choices, model, dataset)\n",
    "get_paths(cache,labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "parse_example(graph_choices)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_activations(activation_patching_result_choices,clean_graph_choices_tokens, dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_activations(activation_patching_result_choices_logits,clean_graph_choices_tokens, dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "imshow([register_pathcing_result_choices[1:]],x=list(range(1,model.cfg.n_layers)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "parse_example(graph_choices)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "random_seed = np.random.randint(1_000_000, 1_000_000_000)\n",
    "graph = generate_example(n_states, random_seed, order=\"backward\")\n",
    "full_path = graph.split(\":\")[1].split(\">\")[1:]  # we ignore the first position, might need to reconsider this at some point\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.reset_hooks()\n",
    "position=46 + 5\n",
    "clean_graph_tokens = torch.from_numpy(dataset.tokenize(graph)[0:-1]).cuda()\n",
    "clean_logits, clean_cache = model.run_with_cache(clean_graph_tokens)\n",
    "print( dataset.untokenize(np.argmax(clean_logits.detach().cpu(),2)[0][47:]))\n",
    "corrupted_graph_tokens= torch.from_numpy(dataset.tokenize(corrupted_graph)[0:-1]).cuda()\n",
    "corrupted_logits, corrupted_cache = model.run_with_cache(corrupted_graph_tokens)\n",
    "activation_patching_result=activation_patching(model, dataset, clean_graph_tokens,corrupted_graph_tokens , position)#46 + 2\n",
    "register_pathcing_result= activation_patching_register(model, dataset, clean_graph_tokens,corrupted_graph_tokens , position,[36,38,39,41,42,44,45])\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "vscode": {
   "interpreter": {
    "hash": "cda9de792cc5e8c03b57ee006a71239929742fe0a549a4cb6551f480ee5c22f9"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
